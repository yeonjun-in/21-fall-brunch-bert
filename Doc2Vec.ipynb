{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3456/3456 [00:32<00:00, 107.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tqdm\n",
    "\n",
    "def iterate_data_files(from_dtm, to_dtm):\n",
    "    from_dtm, to_dtm = map(str, [from_dtm, to_dtm])\n",
    "    read_root = os.path.join('./', 'read')\n",
    "    for fname in os.listdir(read_root):\n",
    "        if len(fname) != len('2018100100_2018100103'):\n",
    "            continue\n",
    "        if from_dtm != 'None' and from_dtm > fname:\n",
    "            continue\n",
    "        if to_dtm != 'None' and fname > to_dtm:\n",
    "            continue\n",
    "        path = os.path.join(read_root, fname)\n",
    "        yield path, fname\n",
    " \n",
    "data = [];\n",
    " \n",
    "files = sorted([path for path, _ in iterate_data_files('2018100100', '2019022200')])\n",
    " \n",
    "for path in tqdm.tqdm(files, mininterval=1):\n",
    "    for line in open(path):\n",
    "        tokens = line.strip().split()\n",
    "        read_datetime = path[7:17]\n",
    "        user_id = tokens[0]\n",
    "        reads = tokens[1:]\n",
    "        for item in reads:\n",
    "            data.append([read_datetime, user_id, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>#e208be4ffea19b1ceb5cea2e3c4dc32c</td>\n",
       "      <td>@kty0613_91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@miamiyoung_31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@banksalad_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@rlfrjsdn_95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@readme999_140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                            user_id      article_id\n",
       "0  2018100100  #e208be4ffea19b1ceb5cea2e3c4dc32c     @kty0613_91\n",
       "1  2018100100  #0a3d493f3b2318be80f391eaa00bfd1c  @miamiyoung_31\n",
       "2  2018100100  #0a3d493f3b2318be80f391eaa00bfd1c   @banksalad_49\n",
       "3  2018100100  #0a3d493f3b2318be80f391eaa00bfd1c    @rlfrjsdn_95\n",
       "4  2018100100  #0a3d493f3b2318be80f391eaa00bfd1c  @readme999_140"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "read_df = pd.DataFrame(data)\n",
    "read_df.columns = ['date', 'user_id', 'article_id']\n",
    " \n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>magazine_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword_list</th>\n",
       "      <th>display_url</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>reg_ts</th>\n",
       "      <th>article_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8982</td>\n",
       "      <td>@bookdb</td>\n",
       "      <td>사진으로 옮기기에도 아까운, 리치필드 국립공원</td>\n",
       "      <td>[여행, 호주, 국립공원]</td>\n",
       "      <td>https://brunch.co.kr/@bookdb/782</td>\n",
       "      <td>세상 어디에도 없는 호주 Top 10</td>\n",
       "      <td>1474944427000</td>\n",
       "      <td>782</td>\n",
       "      <td>@bookdb_782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12081</td>\n",
       "      <td>@kohwang56</td>\n",
       "      <td>[시] 서러운 봄</td>\n",
       "      <td>[목련꽃, 아지랑이, 동행]</td>\n",
       "      <td>https://brunch.co.kr/@kohwang56/81</td>\n",
       "      <td></td>\n",
       "      <td>1463092749000</td>\n",
       "      <td>81</td>\n",
       "      <td>@kohwang56_81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@hannahajink</td>\n",
       "      <td>무엇을 위해</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://brunch.co.kr/@hannahajink/4</td>\n",
       "      <td>무엇 때문에</td>\n",
       "      <td>1447997287000</td>\n",
       "      <td>4</td>\n",
       "      <td>@hannahajink_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16315</td>\n",
       "      <td>@bryceandjuli</td>\n",
       "      <td>싫다</td>\n",
       "      <td>[감정, 마음, 위로]</td>\n",
       "      <td>https://brunch.co.kr/@bryceandjuli/88</td>\n",
       "      <td></td>\n",
       "      <td>1491055161000</td>\n",
       "      <td>88</td>\n",
       "      <td>@bryceandjuli_88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29363</td>\n",
       "      <td>@mijeongpark</td>\n",
       "      <td>Dubliner#7</td>\n",
       "      <td>[유럽여행, 더블린, 아일랜드]</td>\n",
       "      <td>https://brunch.co.kr/@mijeongpark/34</td>\n",
       "      <td>#7. 내 친구의 집은 어디인가</td>\n",
       "      <td>1523292942000</td>\n",
       "      <td>34</td>\n",
       "      <td>@mijeongpark_34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   magazine_id        user_id                      title       keyword_list  \\\n",
       "0         8982        @bookdb  사진으로 옮기기에도 아까운, 리치필드 국립공원     [여행, 호주, 국립공원]   \n",
       "1        12081     @kohwang56                  [시] 서러운 봄    [목련꽃, 아지랑이, 동행]   \n",
       "2            0   @hannahajink                     무엇을 위해                 []   \n",
       "3        16315  @bryceandjuli                         싫다       [감정, 마음, 위로]   \n",
       "4        29363   @mijeongpark                 Dubliner#7  [유럽여행, 더블린, 아일랜드]   \n",
       "\n",
       "                             display_url             sub_title         reg_ts  \\\n",
       "0       https://brunch.co.kr/@bookdb/782  세상 어디에도 없는 호주 Top 10  1474944427000   \n",
       "1     https://brunch.co.kr/@kohwang56/81                        1463092749000   \n",
       "2    https://brunch.co.kr/@hannahajink/4                무엇 때문에  1447997287000   \n",
       "3  https://brunch.co.kr/@bryceandjuli/88                        1491055161000   \n",
       "4   https://brunch.co.kr/@mijeongpark/34     #7. 내 친구의 집은 어디인가  1523292942000   \n",
       "\n",
       "   article_id                id  \n",
       "0         782       @bookdb_782  \n",
       "1          81     @kohwang56_81  \n",
       "2           4    @hannahajink_4  \n",
       "3          88  @bryceandjuli_88  \n",
       "4          34   @mijeongpark_34  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_json('./meta/' + 'metadata.json', lines=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_list</th>\n",
       "      <th>following_list</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[@perytail, @brunch]</td>\n",
       "      <td>#901985d8bc4c481805c4a4f911814c4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[@holidaymemories, @wadiz, @sciforus, @dailydu...</td>\n",
       "      <td>#1fd89e9dcfa64b45020d9eaca54e0eed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[@commerceguy, @sunsutu, @kakao-it, @joohoonja...</td>\n",
       "      <td>#1d94baaea71a831e1f33e1c6bd126ed5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[@amberjeon48, @forsy20, @nemotokki, @hawann, ...</td>\n",
       "      <td>#04641c01892b12dc018b1410e4928c0d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[@dwcha7342, @iammento, @kakao-it, @dkam, @ant...</td>\n",
       "      <td>#65bcaff862aadff877e461f54187ab62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword_list                                     following_list  \\\n",
       "0           []                               [@perytail, @brunch]   \n",
       "1           []  [@holidaymemories, @wadiz, @sciforus, @dailydu...   \n",
       "2           []  [@commerceguy, @sunsutu, @kakao-it, @joohoonja...   \n",
       "3           []  [@amberjeon48, @forsy20, @nemotokki, @hawann, ...   \n",
       "4           []  [@dwcha7342, @iammento, @kakao-it, @dkam, @ant...   \n",
       "\n",
       "                                  id  \n",
       "0  #901985d8bc4c481805c4a4f911814c4a  \n",
       "1  #1fd89e9dcfa64b45020d9eaca54e0eed  \n",
       "2  #1d94baaea71a831e1f33e1c6bd126ed5  \n",
       "3  #04641c01892b12dc018b1410e4928c0d  \n",
       "4  #65bcaff862aadff877e461f54187ab62  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdata = pd.read_json('./meta/' + 'users.json', lines=True)\n",
    "userdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metadata['nouns_list'] = metadata['title'].apply(lambda x: kkma.nouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata['doc_list'] = metadata['user_id'].apply(lambda x: [x]) + metadata['nouns_list'] + metadata['keyword_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/BrunchRec/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = metadata[metadata['id'].isin(read_df['article_id'].unique())].reset_index(drop=True)\n",
    "metadata['doc_list'] = metadata['magazine_id'].apply(lambda x: [str(x)]) + metadata['user_id'].apply(lambda x: [x]) + metadata['keyword_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>rating</th>\n",
       "      <th>magazine_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>id</th>\n",
       "      <th>o_uid</th>\n",
       "      <th>o_sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>1055</td>\n",
       "      <td>1760</td>\n",
       "      <td>1</td>\n",
       "      <td>1189</td>\n",
       "      <td>1914</td>\n",
       "      <td>@hyejinchoi_122</td>\n",
       "      <td>#b8b9d09fe2961fd62edc94912bf75a90</td>\n",
       "      <td>@hyejinchoi_122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>1055</td>\n",
       "      <td>8712</td>\n",
       "      <td>1</td>\n",
       "      <td>1189</td>\n",
       "      <td>1914</td>\n",
       "      <td>@hyejinchoi_112</td>\n",
       "      <td>#b8b9d09fe2961fd62edc94912bf75a90</td>\n",
       "      <td>@hyejinchoi_112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>1267</td>\n",
       "      <td>34504</td>\n",
       "      <td>1</td>\n",
       "      <td>5238</td>\n",
       "      <td>384</td>\n",
       "      <td>@elang8151_154</td>\n",
       "      <td>#5ddc3540a7d1d60e1dfa198787960808</td>\n",
       "      <td>@elang8151_154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>1267</td>\n",
       "      <td>10665</td>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>3200</td>\n",
       "      <td>@7-words_12</td>\n",
       "      <td>#5ddc3540a7d1d60e1dfa198787960808</td>\n",
       "      <td>@7-words_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018100100</td>\n",
       "      <td>1267</td>\n",
       "      <td>18639</td>\n",
       "      <td>1</td>\n",
       "      <td>2376</td>\n",
       "      <td>4236</td>\n",
       "      <td>@windyroad2_122</td>\n",
       "      <td>#5ddc3540a7d1d60e1dfa198787960808</td>\n",
       "      <td>@windyroad2_122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp   uid    sid  rating  magazine_id  user_id               id  \\\n",
       "0  2018100100  1055   1760       1         1189     1914  @hyejinchoi_122   \n",
       "1  2018100100  1055   8712       1         1189     1914  @hyejinchoi_112   \n",
       "2  2018100100  1267  34504       1         5238      384   @elang8151_154   \n",
       "3  2018100100  1267  10665       1          620     3200      @7-words_12   \n",
       "4  2018100100  1267  18639       1         2376     4236  @windyroad2_122   \n",
       "\n",
       "                               o_uid            o_sid  \n",
       "0  #b8b9d09fe2961fd62edc94912bf75a90  @hyejinchoi_122  \n",
       "1  #b8b9d09fe2961fd62edc94912bf75a90  @hyejinchoi_112  \n",
       "2  #5ddc3540a7d1d60e1dfa198787960808   @elang8151_154  \n",
       "3  #5ddc3540a7d1d60e1dfa198787960808      @7-words_12  \n",
       "4  #5ddc3540a7d1d60e1dfa198787960808  @windyroad2_122  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "amap = dict(df[['o_sid', 'user_id']].drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(metadata['doc_list'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['16315', '@bryceandjuli', '감정', '마음', '위로'], tags=['0'])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(tagged_data, vector_size=100, window=15, min_count=1, \n",
    "                workers=4, alpha=0.001, min_alpha=0.0001, dm=1, epochs=20, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "amap = dict(df[['o_sid', 'user_id']].drop_duplicates().values)\n",
    "metadata['sid'] = metadata['id'].apply(lambda x: amap[x] if x in amap else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['index'] = metadata.index\n",
    "metadata_p = metadata[metadata['sid'].notnull()].reset_index(drop=True)\n",
    "imap = dict(metadata_p[['index', 'sid']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34612/34612 [12:38<00:00, 45.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "doc2vec_list = dict()\n",
    "for doc_index in tqdm(metadata[metadata['sid'].notnull()].index): \n",
    "    new_vector = model.infer_vector(metadata['doc_list'][doc_index])\n",
    "    sid = metadata['sid'][doc_index]\n",
    "    \n",
    "    rec_list = []\n",
    "    for i, j in model.docvecs.most_similar([new_vector], topn=1000): \n",
    "        if int(i) in imap: \n",
    "            rec_list += [(int(imap[int(i)]), j)]\n",
    "    rec_list = rec_list[0:100]\n",
    "    rec_dict = dict(rec_list)\n",
    "    doc2vec_list[int(sid)] = rec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Tag2Vec.pkl', 'wb') as write_file:\n",
    "    pickle.dump(doc2vec_list, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineItemSimilarity:\n",
    "    def __init__(self, data_file=None, similarity_path=None, model_name='ItemCF', \\\n",
    "        dataset_name='Sports_and_Outdoors'):\n",
    "        self.similarity_path = similarity_path\n",
    "        self.model_name = model_name\n",
    "        self.similarity_model = self.load_similarity_model(self.similarity_path)\n",
    "        self.max_score, self.min_score = self.get_maximum_minimum_sim_scores()\n",
    "        \n",
    "    def get_maximum_minimum_sim_scores(self):\n",
    "        max_score, min_score = -1, 100\n",
    "        for item in self.similarity_model.keys():\n",
    "            for neig in self.similarity_model[item]:\n",
    "                sim_score = self.similarity_model[item][neig]\n",
    "                max_score = max(max_score, sim_score)\n",
    "                min_score = min(min_score, sim_score)\n",
    "        return max_score, min_score\n",
    "    \n",
    "    def _convert_data_to_dict(self, data):\n",
    "        \"\"\"\n",
    "        split the data set\n",
    "        testdata is a test data set\n",
    "        traindata is a train set\n",
    "        \"\"\"\n",
    "        train_data_dict = {}\n",
    "        for user,item,record in data:\n",
    "            train_data_dict.setdefault(user,{})\n",
    "            train_data_dict[user][item] = record\n",
    "        return train_data_dict\n",
    "\n",
    "    def _save_dict(self, dict_data, save_path = './similarity.pkl'):\n",
    "        print(\"saving data to \", save_path)\n",
    "        with open(save_path, 'wb') as write_file:\n",
    "            pickle.dump(dict_data, write_file)\n",
    "\n",
    "    def load_similarity_model(self, similarity_model_path):\n",
    "        if not similarity_model_path:\n",
    "            raise ValueError('invalid path')\n",
    "        elif not os.path.exists(similarity_model_path):\n",
    "            print(\"the similirity dict not exist, generating...\")\n",
    "            self._generate_item_similarity(save_path=self.similarity_path)\n",
    "        if self.model_name in ['ItemCF', 'ItemCF_IUF', 'Item2Vec', 'LightGCN', 'Tag2Vec']:\n",
    "            with open(similarity_model_path, 'rb') as read_file:\n",
    "                similarity_dict = pickle.load(read_file)\n",
    "            return similarity_dict\n",
    "        elif self.model_name == 'Random':\n",
    "            similarity_dict = self.train_item_list\n",
    "            return similarity_dict\n",
    "\n",
    "    def most_similar(self, item, top_k=1, with_score=False):\n",
    "        if self.model_name in ['ItemCF', 'ItemCF_IUF', 'Item2Vec', 'LightGCN', 'Tag2Vec']:\n",
    "            \"\"\"TODO: handle case that item not in keys\"\"\"\n",
    "            if str(item) in self.similarity_model:\n",
    "                top_k_items_with_score = sorted(self.similarity_model[str(item)].items(),key=lambda x : x[1], \\\n",
    "                                            reverse=True)[0:top_k]\n",
    "                if with_score:\n",
    "                    return list(map(lambda x: (int(x[0]), (self.max_score - float(x[1]))/(self.max_score -self.min_score)), top_k_items_with_score))\n",
    "                return list(map(lambda x: int(x[0]), top_k_items_with_score))\n",
    "            elif int(item) in self.similarity_model:\n",
    "                top_k_items_with_score = sorted(self.similarity_model[int(item)].items(),key=lambda x : x[1], \\\n",
    "                                            reverse=True)[0:top_k]\n",
    "                if with_score:\n",
    "                    return list(map(lambda x: (int(x[0]), (self.max_score - float(x[1]))/(self.max_score -self.min_score)), top_k_items_with_score))\n",
    "                return list(map(lambda x: int(x[0]), top_k_items_with_score))\n",
    "            else:\n",
    "                item_list = list(self.similarity_model.keys())\n",
    "                random_items = random.sample(item_list, k=top_k)\n",
    "                if with_score:\n",
    "                    return list(map(lambda x: (int(x), 0.0), random_items))\n",
    "                return list(map(lambda x: int(x), random_items))\n",
    "        elif self.model_name == 'Random':\n",
    "            random_items = random.sample(self.similarity_model, k = top_k)\n",
    "            if with_score:\n",
    "                return list(map(lambda x: (int(x), 0.0), random_items))\n",
    "            return list(map(lambda x: int(x), random_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "similaritymodel = OfflineItemSimilarity(similarity_path='Tag2Vec.pkl', model_name='Tag2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3893, 1056, 353, 296, 1067, 84, 3912, 3742, 2931, 3066]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "similaritymodel.most_similar(0, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f608861d47515b19ed2972810bb0023a27e2ef9c0a4c2425f98b0eb75a3a25e4"
  },
  "kernelspec": {
   "display_name": "Python [conda env:BrunchRec]",
   "language": "python",
   "name": "conda-env-BrunchRec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
